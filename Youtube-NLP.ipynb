{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Reshape, Attention, TimeDistributed, Embedding, LSTM, Bidirectional, Dropout, Dense, GRU, concatenate, Conv1D, Conv2D, Flatten,MaxPooling1D, MaxPooling2D, LocallyConnected1D, Activation, GaussianNoise, BatchNormalization, RepeatVector\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from AttentionWeightedAverage import AttentionWeightedAverage\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP on Title + Description to Tags, in order to build a tag recommendation algorithm\n",
    "- NLP on Tags + Description + Category to Title, in order to provide title recommendation algorithm. These videos are popular videos, meaning they have appealing titles. Let's build a system, that judging by content, can provide the most appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of youtube videos in dataframe:  40949\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/USvideos.csv\")\n",
    "print('Number of youtube videos in dataframe: ', len(df.index))\n",
    "df.head()\n",
    "\n",
    "# count category occurences\n",
    "occurences = df['category_id'].value_counts()\n",
    "# category names\n",
    "map_categories = {}\n",
    "with open('data/US_category_id.json') as json_file:\n",
    "    data_category = json.load(json_file)\n",
    "    for category in data_category['items']:\n",
    "        int_id = int(category['id'])\n",
    "        occ = 0\n",
    "        if int_id in occurences:\n",
    "            occ = occurences[int_id]\n",
    "        map_categories[int_id] = [category['snippet']['title'],occ]\n",
    "# create new dataframe with categories and appearances\n",
    "df_categories = pd.DataFrame.from_dict(map_categories, orient='index', columns=['category_name', 'count'])\n",
    "# add category name on the main dataframe\n",
    "for i in map_categories.keys():\n",
    "    df.loc[df['category_id'] == i,'category_name'] = map_categories[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags, Description and Category -> Auto generate Title\n",
    "We will see a word level seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(tags):\n",
    "    tags = tags.replace('\"','')\n",
    "    discrete_tags = tags.split('|')\n",
    "    return discrete_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['title', 'description', 'tags', 'category_name']]\n",
    "df_text = df_text.dropna(axis=0, how='any')\n",
    "\n",
    "titles = df_text['title'].tolist()\n",
    "descriptions = df_text['description'].tolist()\n",
    "tags = [discretize(row['tags']) for index, row in df_text.iterrows()]\n",
    "categories = df_text['category_name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Titles need to be tokenized, lowercased, categorical.\n",
    "- Descriptions need to be tokenized, lowercased, categorical.\n",
    "- Tags need to be lowercased, categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_desc = []\n",
    "clean_title = []\n",
    "for desc in descriptions:\n",
    "    desc = desc.replace('\\\\n', ' ')\n",
    "    desc = desc.lower()\n",
    "    desc = desc.replace('www.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('http.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('www.twitter.com', 'TwitterLink')\n",
    "    desc = desc.replace('www.amazon.com', 'AmazonLink')\n",
    "    desc = desc.replace('http.amzn.com', 'AmazonLink')\n",
    "    desc = desc.replace('www.instagram.com', 'InstagramLink')\n",
    "    desc = desc.replace('www.snapchat.com', 'SnapchatLink')\n",
    "    desc = desc.replace('https://bit.ly', 'PortalLink')\n",
    "    temp = text_to_word_sequence(desc, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' ')\n",
    "    for w in temp:\n",
    "        if w in stops:\n",
    "            temp.remove(w)\n",
    "    clean_desc.append(temp)\n",
    "    \n",
    "for title in titles:\n",
    "    title = title.replace('\\\\n', ' ')\n",
    "    title = title.replace('\\xa0', ' ')\n",
    "    seq = text_to_word_sequence(title, filters='\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' ')\n",
    "    clean_title.append(['START_ '] + seq + [' _END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an unsupervised method for word2vec with gensim, in order to provide me with very good embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57577047, 64014370)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "input_features = 100\n",
    "\n",
    "gensim_model = Word2Vec(size=input_features,\n",
    "                       window=5,\n",
    "                       min_count=1,\n",
    "                       sg=1,\n",
    "                       hs=0,\n",
    "                       negative=10,\n",
    "                       workers=4)\n",
    "gensim_model.build_vocab(clean_desc+clean_title+tags)\n",
    "gensim_model.train(clean_desc+clean_title+tags, total_examples=len(clean_desc+clean_title+tags), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the vocabulary for the words that appear in the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title_words=set()\n",
    "for title in clean_title:\n",
    "    for word in title:\n",
    "        if word not in all_title_words:\n",
    "            all_title_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder and decoder tokens defer, the encoder takes words from the description, while the decoder produces words from the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(gensim_model.wv.vocab)\n",
    "\n",
    "target_words = sorted(list(all_title_words))\n",
    "num_decoder_tokens = len(all_title_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating my new sequences with vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_description = []\n",
    "X_train_category = []\n",
    "X_train_tags = []\n",
    "\n",
    "lb_cat = LabelBinarizer()\n",
    "lb_cat.fit(list(set(categories)))\n",
    "\n",
    "maxLenDesc = 0\n",
    "for desc in clean_desc:\n",
    "    X_train_description.append(np.asarray([gensim_model.wv[word] for word in desc]))\n",
    "    if len(desc) > maxLenDesc:\n",
    "        maxLenDesc = len(desc)\n",
    "        \n",
    "maxLenTags = 0\n",
    "for taglist in tags:\n",
    "    X_train_tags.append(np.asarray([gensim_model.wv[tag] for tag in taglist]))\n",
    "    if len(taglist) > maxLenTags:\n",
    "        maxLenTags = len(taglist)\n",
    "\n",
    "for cat in categories:\n",
    "    X_train_category.append(lb_cat.transform([cat]))\n",
    "\n",
    "maxLenTitle = 0\n",
    "for title in clean_title:\n",
    "    if len(title) > maxLenTitle:\n",
    "        maxLenTitle = len(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the decoder inputs and outputs. Decoder target data will be ahead by one timestep, and will not include the start character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = np.zeros(\n",
    "    (len(clean_title), maxLenTitle, input_features),\n",
    "    dtype='float32')\n",
    "\n",
    "Y_train_title = np.zeros(\n",
    "    (len(clean_title), maxLenTitle, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# generate data\n",
    "for i, target_text in enumerate(clean_title):\n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder input data, will have the embeddings of the words\n",
    "        X_train_title[i, t] = gensim_model.wv[word]\n",
    "        if t > 0:\n",
    "            # decoder target data, will be ahead by one timestep, and will contain\n",
    "            # one hot representations of what word is selected\n",
    "            Y_train_title[i, t - 1, target_words.index(word)] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_train_description = pad_sequences(X_train_description, maxlen=maxLenDesc, padding='post')\n",
    "X_train_tags = pad_sequences(X_train_tags, maxlen=maxLenTags, padding='post')\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train_description = np.stack(X_train_description, axis=0)\n",
    "X_train_category = np.stack(X_train_category, axis=0)\n",
    "X_train_tags = np.stack(X_train_tags, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Test Split\n",
    "* Encoder inputs\n",
    "* Decoder inputs + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder + decoder inputs\n",
    "X_train_description, X_test_description, X_train_title, X_test_title = train_test_split(X_train_description, X_train_title, test_size=0.05, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_titles = len(clean_title)\n",
    "test_size = X_test_description.shape[0]\n",
    "# decoder outputs\n",
    "Y_train_title, Y_test_title = (Y_train_title[:num_titles-test_size], Y_train_title[num_titles-test_size:num_titles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2SeqModel(vocab_size,input_max_seq,output_target_seq,input_features):\n",
    "    #encoder\n",
    "    encoder_input = Input(shape=(input_max_seq,input_features), name='main_input')\n",
    "    encoder_lstm = LSTM(100, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_input)\n",
    "    encoder_states = [encoder_state_h, encoder_state_c]\n",
    "    \n",
    "    #decoder\n",
    "    decoder_input = Input(shape=(None,input_features))\n",
    "    decoder_lstm = LSTM(100, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(vocab_size, name='decoder_dense', activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=4e-3), metrics=['accuracy'])\n",
    "    # encoder model\n",
    "    encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "    decoder_state_inputs = [Input(shape=(100,)), Input(shape=(100,))]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model,encoder_model,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         [(None, 722, 100)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 100), (None, 80400       main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 100),  80400       input_7[0][0]                    \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 11444)  1155844     decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,316,644\n",
      "Trainable params: 1,316,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "batch_size=128\n",
    "model,encoder_model,decoder_model = Seq2SeqModel(num_decoder_tokens,maxLenDesc,maxLenTitle,100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38360 samples\n",
      "Epoch 1/2\n",
      "38360/38360 [==============================] - 812s 21ms/sample - loss: 2.8617 - accuracy: 0.6146\n",
      "Epoch 2/2\n",
      "38360/38360 [==============================] - 855s 22ms/sample - loss: 2.2052 - accuracy: 0.6242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199c9ec0348>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_description, X_train_title], Y_train_title, epochs=epochs, batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = gensim_model.wv['START_ ']\n",
    "    target_seq = target_seq.reshape((1,1,target_seq.shape[0]))\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, states_value[0], states_value[1]])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = target_words[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == ' _END' or len(decoded_sentence) > maxLenTitle):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.append(target_seq, gensim_model.wv[sampled_word].reshape((1,1,input_features)), axis=1)\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Description:\n",
      " SHANTELL'S CHANNEL - https://www.youtube.com/shantellmartin\\nCANDICE - https://www.lovebilly.com\\n\\nfilmed this video in 4k on this -- http://amzn.to/2sTDnRZ\\nwith this lens -- http://amzn.to/2rUJOmD\\nbig drone - http://tinyurl.com/h4ft3oy\\nOTHER GEAR ---  http://amzn.to/2o3GLX5\\nSony CAMERA http://amzn.to/2nOBmnv\\nOLD CAMERA; http://amzn.to/2o2cQBT\\nMAIN LENS; http://amzn.to/2od5gBJ\\nBIG SONY CAMERA; http://amzn.to/2nrdJRO\\nBIG Canon CAMERA; http://tinyurl.com/jn4q4vz\\nBENDY TRIPOD THING; http://tinyurl.com/gw3ylz2\\nYOU NEED THIS FOR THE BENDY TRIPOD; http://tinyurl.com/j8mzzua\\nWIDE LENS; http://tinyurl.com/jkfcm8t\\nMORE EXPENSIVE WIDE LENS; http://tinyurl.com/zrdgtou\\nSMALL CAMERA; http://tinyurl.com/hrrzhor\\nMICROPHONE; http://tinyurl.com/zefm4jy\\nOTHER MICROPHONE; http://tinyurl.com/jxgpj86\\nOLD DRONE (cheaper but still great);http://tinyurl.com/zcfmnmd\\n\\nfollow me; on http://instagram.com/caseyneistat\\non https://www.facebook.com/cneistat\\non https://twitter.com/CaseyNeistat\\n\\namazing intro song by https://soundcloud.com/discoteeth\\n\\nad disclosure.  THIS IS NOT AN AD.  not selling or promoting anything.  but samsung did produce the Shantell Video as a 'GALAXY PROJECT' which is an initiative that enables creators like Shantell and me to make projects we might otherwise not have the opportunity to make.  hope that's clear.  if not ask in the comments and i'll answer any specifics.\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  1\n",
      "Description:\n",
      " One year after the presidential election, John Oliver discusses what we've learned so far and enlists our catheter cowboy to teach Donald Trump what he hasn't.\\n\\nConnect with Last Week Tonight online...\\n\\nSubscribe to the Last Week Tonight YouTube channel for more almost news as it almost happens: www.youtube.com/user/LastWeekTonight\\n\\nFind Last Week Tonight on Facebook like your mom would: http://Facebook.com/LastWeekTonight\\n\\nFollow us on Twitter for news about jokes and jokes about news: http://Twitter.com/LastWeekTonight\\n\\nVisit our official site for all that other stuff at once: http://www.hbo.com/lastweektonight\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  2\n",
      "Description:\n",
      " WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► https://www.youtube.com/channel/UC5jkXpfnBhlDjqh0ir5FsIQ?sub_confirmation=1\\n\\nTHANKS FOR WATCHING! LIKE & SUBSCRIBE FOR MORE VIDEOS!\\n-----------------------------------------------------------\\nFIND ME ON: \\nInstagram | http://instagram.com/rudymancuso\\nTwitter | http://twitter.com/rudymancuso\\nFacebook | http://facebook.com/rudymancuso\\n\\nCAST: \\nRudy Mancuso | http://youtube.com/c/rudymancuso\\nLele Pons | http://youtube.com/c/lelepons\\nKing Bach | https://youtube.com/user/BachelorsPadTv\\n\\nVideo Effects: \\nCaleb Natale | https://instagram.com/calebnatale\\n\\nPA:\\nPaulina Gregory\\n\\n\\nShots Studios Channels:\\nAlesso | https://youtube.com/c/alesso\\nAnitta | http://youtube.com/c/anitta\\nAnwar Jibawi | http://youtube.com/c/anwar\\nAwkward Puppets | http://youtube.com/c/awkwardpuppets\\nHannah Stocking | http://youtube.com/c/hannahstocking\\nInanna Sarkis | http://youtube.com/c/inanna\\nLele Pons | http://youtube.com/c/lelepons\\nMaejor | http://youtube.com/c/maejor\\nMike Tyson | http://youtube.com/c/miketyson \\nRudy Mancuso | http://youtube.com/c/rudymancuso\\nShots Studios | http://youtube.com/c/shots\\n\\n#Rudy\\n#RudyMancuso\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  3\n",
      "Description:\n",
      " Today we find out if Link is a Nickelback amateur or a secret Nickelback devotee. GMM #1218\\nDon't miss an all new Ear Biscuits: https://goo.gl/xeZNQt\\nWatch Part 4: https://youtu.be/MhCdiiB8CQg | Watch Part 2: https://youtu.be/7qiOrNao9fg\\nWatch today's episode from the start: http://bit.ly/GMM1218\\n\\nPick up all of the official GMM merch only at https://mythical.store\\n\\nFollow Rhett & Link: \\nInstagram: https://instagram.com/rhettandlink\\nFacebook: https://facebook.com/rhettandlink\\nTwitter: https://twitter.com/rhettandlink\\nTumblr: https://rhettandlink.tumblr.com\\nSnapchat: @realrhettlink\\nWebsite: https://mythical.co/\\n\\nCheck Out Our Other Mythical Channels:\\nGood Mythical MORE: https://youtube.com/goodmythicalmore\\nRhett & Link: https://youtube.com/rhettandlink\\nThis Is Mythical: https://youtube.com/thisismythical\\nEar Biscuits: https://applepodcasts.com/earbiscuits\\n\\nWant to send us something? https://mythical.co/contact\\nHave you made a Wheel of Mythicality intro video? Submit it here: https://bit.ly/GMMWheelIntro\\n\\nIntro Animation by Digital Twigs: https://www.digitaltwigs.com\\nIntro & Outro Music by Jeff Zeigler & Sarah Schimeneck https://www.jeffzeigler.com\\nWheel of Mythicality theme: https://www.royaltyfreemusiclibrary.com/\\nAll Supplemental Music fromOpus 1 Music: https://opus1.sourceaudio.com/\\nWe use ‘The Mouse’ by Blue Microphones https://www.bluemic.com/mouse/\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  4\n",
      "Description:\n",
      " I know it's been a while since we did this show, but we're back with what might be the best episode yet!\\nLeave your dares in the comment section! \\n\\nOrder my book how to write good \\nhttp://higatv.com/ryan-higas-how-to-write-good-pre-order-links/\\n\\nJust Launched New Official Store\\nhttps://www.gianthugs.com/collections/ryan\\n\\nHigaTV Channel\\nhttp://www.youtube.com/higatv\\n\\nTwitter\\nhttp://www.twitter.com/therealryanhiga\\n\\nFacebook\\nhttp://www.facebook.com/higatv\\n\\nWebsite\\nhttp://www.higatv.com\\n\\nInstagram\\nhttp://www.instagram.com/notryanhiga\\n\\nSend us mail or whatever you want here!\\nPO Box 232355\\nLas Vegas, NV 89105\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  5\n",
      "Description:\n",
      " Using the iPhone for the past two weeks -- here's my thoughts!\\nAll my iPhone X Videos: https://www.youtube.com/watch?v=vqztGUwhRlQ&list=PLoYRB6C09WUDbCndtEDELX-Fpk_pgATvF\\n► SUBSCRIBE FOR MORE VIDEOS: http://www.youtube.com/subscription_center?add_user=ijustine\\n► Get my BOOK! http://ijustinebook.com\\n► Get my iOS 10 STICKERS! http://ijustinestickers.com\\n\\n▼ SOCIAL\\nhttp://instagram.com/ijustine\\nhttp://facebook.com/ijustine\\nhttp://twitter.com/ijustine\\nSnapchat: iJustine\\n\\n————————————\\n\\n▼ STUFF I USE TO MAKE VIDEOS\\n\\nSony rx100 V - http://amzn.to/2jesbxA\\nG7X ii - http://amzn.to/2f6n2Bs\\nCanon 80D - http://amzn.to/2eRKhQo\\nSony A7s ii - http://amzn.to/2ebLR16\\nSony a6500 - http://amzn.to/2okeG2a\\nGoPro Hero 5 - http://amzn.to/2e1KyhM\\nGoPro Hero 5 Session - http://amzn.to/2oksMQT\\nEpidemic Sound - https://goo.gl/Pe7GTL \\n\\nFavorite lenses: \\nCanon EF 24-70mm - http://amzn.to/2dT7mFr\\nCanon EF-S 10-18mm - http://amzn.to/2dT62SU\\nSony 16-35mm (full frame) - http://amzn.to/2ftPaTf\\nSony Distagon 35mm (full frame) - http://amzn.to/2oB0XQj\\nSony 10-18mm wide angle - http://amzn.to/2e1Myqz\\n\\nRode Small on camera microphone - http://amzn.to/2fkiVGJ\\nRode Larger (battery required) microphone - http://amzn.to/2ftNkl8\\nSony XLR adapter Microphone - http://amzn.to/2kCcIDH\\nSmall Sony Microphone - http://amzn.to/2oX7Eih\\n\\nFavorite SD Card - http://amzn.to/2oWRGoD\\n\\nDJI Mavic Pro - http://amzn.to/2f6nL5E\\nPhantom 4 Pro - http://amzn.to/2pbDrN1\\nPhantom 4 Pro Plus  - http://amzn.to/2oX63Jz\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  6\n",
      "Description:\n",
      " Embattled Alabama Senate candidate Roy Moore (Mikey Day) meets with Vice President Mike Pence (Beck Bennett) and Attorney General Jeff Sessions (Kate McKinnon).\\n\\n#SNL #SNL43\\n\\nGet more SNL: http://www.nbc.com/saturday-night-live\\nFull Episodes: http://www.nbc.com/saturday-night-liv...\\n\\nLike SNL: https://www.facebook.com/snl\\nFollow SNL: https://twitter.com/nbcsnl\\nSNL Tumblr: http://nbcsnl.tumblr.com/\\nSNL Instagram: http://instagram.com/nbcsnl \\nSNL Pinterest: http://www.pinterest.com/nbcsnl/\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  7\n",
      "Description:\n",
      " Ice Cream Pint Combination Lock - http://amzn.to/2ACipdI\\nMini Ice Cream Sandwich Maker - http://amzn.to/2ACiX3g\\nMotorized Ice Cream Cone - http://amzn.to/2jkrYdg\\nBall Ice Cream Maker - http://amzn.to/2AEbtwE\\nScoop and Stack - http://amzn.to/2zEMhGo\\n\\nSubscribe to: \\n2nd channel - https://www.youtube.com/user/origami768\\ninstagram https://instagram.com/crazyrussianhacker/\\nfacebook - https://www.facebook.com/CrazyRussianHacker\\n\\nDON'T TRY THIS AT HOME!\\n\\nBusiness email: crh.inquire@gmail.com\\n\\nFAN MAIL:  \\nCRAZY RUSSIAN HACKER\\nP.O. Box 49\\nWaynesville, NC 28786\\n\\nDISCLAIMER: In this video description contains affiliate links, which means that if you click on one of the product links, I’ll receive a small commission.\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  8\n",
      "Description:\n",
      " Inspired by the imagination of P.T. Barnum, The Greatest Showman is an original musical that celebrates the birth of show business & tells of a visionary who rose from nothing to create a spectacle that became a worldwide sensation.\\n\\nIn Theaters This Christmas\\n\\nDirected By Michael Gracey\\nCast: Hugh Jackman, Michelle Williams, Zac Efron, Zendaya, Rebecca Ferguson\\n\\nSUBSCRIBE: http://bit.ly/FOXSubscribe\\n\\n\\nVisit the Official Site Here: http://www.foxmovies.com/movies/the-greatest-showman\\nLike The Greatest Showman on FACEBOOK: http://fox.co/GreatestShowmanFB\\nFollow The Greatest Showman on TWITTER: http://fox.co/GreatestShowmanTW\\nFollow The Greatest Showman on INSTAGRAM: http://fox.co/GreatestShowmanIG\\n\\n#GreatestShowman\\n\\nAbout 20th Century FOX:\\nOfficial YouTube Channel for 20th Century Fox Movies. Home of Avatar, Aliens, X-Men, Die Hard, Deadpool, Ice Age, Alvin and the Chipmunks, Rio, Peanuts, Maze Runner, Planet of the Apes, Wolverine and many more.\\n\\nConnect with 20th Century FOX Online:\\nVisit the 20th Century FOX WEBSITE: http://bit.ly/FOXMovie\\nLike 20th Century FOX on FACEBOOK: http://bit.ly/FOXFacebook\\nFollow 20th Century FOX on TWITTER: http://bit.ly/TwitterFOX\\n\\nThe Greatest Showman | Official Trailer 2 [HD] | 20th Century FOX\\nhttp://www.youtube.com/user/FoxMovies\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n",
      "i:  9\n",
      "Description:\n",
      " For now, at least, we have better things to worry about.\\n\\n\\nSubscribe to our channel! http://goo.gl/0bsAjO\\n\\nSources: \\nhttps://economics.mit.edu/files/11563\\nhttps://www.aeaweb.org/full_issue.php?doi=10.1257/jep.29.3#page=33\\nhttp://voxeu.org/article/how-computer-automation-affects-occupations\\nhttps://www.opensocietyfoundations.org/sites/default/files/future-work-lit-review-20150428.pdf\\nhttps://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial-Intelligence-Automation-Economy.PDF\\nhttps://www.vox.com/2015/7/27/9038829/automation-myth\\nhttps://www.amazon.com/dp/B00PWX7RPG/ref=dp-kindle-redirect\\nhttps://www.amazon.com/Second-Machine-Age-Prosperity-Technologies-ebook/dp/B00D97HPQI/ref=sr_1_1\\nhttps://www.amazon.com/New-Division-Labor-Computers-Creating/dp/0691124027/ref=sr_1_1?\\nhttps://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf\\n\\nClips:\\nhttps://www.youtube.com/watch?v=VTlV0Y5yAww\\nhttps://www.youtube.com/watch?v=_luhn7TLfWU\\nhttps://www.youtube.com/watch?v=rVlhMGQgDkY\\nhttps://www.youtube.com/watch?v=rCoFKUJ_8Yo\\nhttps://www.youtube.com/watch?v=yeyn9zzrC84\\nhttps://www.youtube.com/watch?v=7Pq-S557XQU\\nhttps://www.youtube.com/watch?v=WSKi8HfcxEk\\n\\n///\\n\\nRecent advancements in artificial intelligence and robotics have commentators worrying about the coming obsolescence of the human worker. Some in Silicon Valley are even calling for a basic minimum income provided by the government for everyone, under the assumption that work will become scarce. But many economists are skeptical of these claims, because the notion that the the economy offers a fixed amount of work has been debunked time and time again over the centuries and current economic data show no signs of a productivity boom. Fortunately, we don't need to divine the future of the labor market in order to prepare for it. \\n\\n///\\n\\nVox.com is a news website that helps you cut through the noise and understand what's really driving the events in the headlines. Check out http://www.vox.com to get up to speed on everything from Kurdistan to the Kim Kardashian app. \\n\\nCheck out our full video catalog: http://goo.gl/IZONyE\\nFollow Vox on Twitter: http://goo.gl/XFrZ5H\\nOr on Facebook: http://goo.gl/U2g06o\n",
      "\n",
      "Generated Title:   the voice of the world\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for i in list(range(10)):\n",
    "    title_generated = decode_sequence(X_test_description[i].reshape((1,X_test_description.shape[1],X_test_description.shape[2])))\n",
    "    print(\"i: \", i)\n",
    "    print(\"Description:\\n\",descriptions[i])\n",
    "    print(\"\\nGenerated Title: \", title_generated)\n",
    "    print(\"\\n\\n\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
