{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Reshape, Attention, TimeDistributed, Embedding, LSTM, Bidirectional, Dropout, Dense, GRU, concatenate, Conv1D, Conv2D, Flatten,MaxPooling1D, MaxPooling2D, LocallyConnected1D, Activation, GaussianNoise, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import nltk\n",
    "from AttentionWeightedAverage import AttentionWeightedAverage\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP on Title + Description to Tags, in order to build a tag recommendation algorithm\n",
    "- NLP on Tags + Description + Category to Title, in order to provide title recommendation algorithm. These videos are popular videos, meaning they have appealing titles. Let's build a system, that judging by content, can provide the most appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of youtube videos in dataframe:  40949\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/USvideos.csv\")\n",
    "print('Number of youtube videos in dataframe: ', len(df.index))\n",
    "df.head()\n",
    "\n",
    "# count category occurences\n",
    "occurences = df['category_id'].value_counts()\n",
    "# category names\n",
    "map_categories = {}\n",
    "with open('data/US_category_id.json') as json_file:\n",
    "    data_category = json.load(json_file)\n",
    "    for category in data_category['items']:\n",
    "        int_id = int(category['id'])\n",
    "        occ = 0\n",
    "        if int_id in occurences:\n",
    "            occ = occurences[int_id]\n",
    "        map_categories[int_id] = [category['snippet']['title'],occ]\n",
    "# create new dataframe with categories and appearances\n",
    "df_categories = pd.DataFrame.from_dict(map_categories, orient='index', columns=['category_name', 'count'])\n",
    "# add category name on the main dataframe\n",
    "for i in map_categories.keys():\n",
    "    df.loc[df['category_id'] == i,'category_name'] = map_categories[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags, Description and Category -> Auto generate Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(tags):\n",
    "    tags = tags.replace('\"','')\n",
    "    discrete_tags = tags.split('|')\n",
    "    return discrete_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['title', 'description', 'tags', 'category_name']]\n",
    "df_text = df_text.dropna(axis=0, how='any')\n",
    "\n",
    "titles = df_text['title'].tolist()\n",
    "descriptions = df_text['description'].tolist()\n",
    "tags = [discretize(row['tags']) for index, row in df_text.iterrows()]\n",
    "categories = df_text['category_name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Titles need to be tokenized, lowercased, categorical.\n",
    "- Descriptions need to be tokenized, lowercased, categorical.\n",
    "- Tags need to be lowercased, categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_desc = []\n",
    "clean_title = []\n",
    "for desc in descriptions:\n",
    "    desc = desc.replace('\\\\n', ' ')\n",
    "    desc = desc.lower()\n",
    "    desc = desc.replace('www.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('http.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('www.twitter.com', 'TwitterLink')\n",
    "    desc = desc.replace('www.amazon.com', 'AmazonLink')\n",
    "    desc = desc.replace('http.amzn.com', 'AmazonLink')\n",
    "    desc = desc.replace('www.instagram.com', 'InstagramLink')\n",
    "    desc = desc.replace('www.snapchat.com', 'SnapchatLink')\n",
    "    desc = desc.replace('https://bit.ly', 'PortalLink')\n",
    "    clean_desc.append(text_to_word_sequence(desc, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' '))\n",
    "    \n",
    "for title in titles:\n",
    "    title = title.replace('\\\\n', ' ')\n",
    "    seq = text_to_word_sequence(title, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' ')\n",
    "    token_seq = []\n",
    "    for w in seq:\n",
    "        token_seq += list(w)\n",
    "        token_seq += \" \"\n",
    "    clean_title.append(token_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an unsupervised method for word2vec with gensim, in order to provide me with very good embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70696706, 91275720)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "gensim_model = Word2Vec(size=100,\n",
    "                       window=5,\n",
    "                       min_count=1,\n",
    "                       sg=1,\n",
    "                       hs=0,\n",
    "                       negative=10,\n",
    "                       workers=4)\n",
    "gensim_model.build_vocab(clean_desc+clean_title+tags)\n",
    "gensim_model.train(clean_desc+clean_title+tags, total_examples=len(clean_desc+clean_title+tags), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating my new sequences with vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_description = []\n",
    "X_train_category = []\n",
    "X_train_tags = []\n",
    "Y_train_title = []\n",
    "\n",
    "lb_cat = LabelBinarizer()\n",
    "lb_cat.fit(list(set(categories)))\n",
    "\n",
    "maxLenDesc = 0\n",
    "for desc in clean_desc:\n",
    "    X_train_description.append(np.asarray([gensim_model.wv[word] for word in desc]))\n",
    "    if len(desc) > maxLenDesc:\n",
    "        maxLenDesc = len(desc)\n",
    "        \n",
    "maxLenTags = 0\n",
    "for taglist in tags:\n",
    "    X_train_tags.append(np.asarray([gensim_model.wv[tag] for tag in taglist]))\n",
    "    if len(taglist) > maxLenTags:\n",
    "        maxLenTags = len(taglist)\n",
    "\n",
    "for cat in categories:\n",
    "    X_train_category.append(lb_cat.transform([cat]))\n",
    "\n",
    "vocab_chars = {}\n",
    "maxLenTitle = 0\n",
    "vocab_char_size = 0\n",
    "for title in clean_title:\n",
    "    for ch in title:\n",
    "        if ch not in vocab_chars.keys():\n",
    "            vocab_chars[ch] = vocab_char_size\n",
    "            vocab_char_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_title = LabelBinarizer()\n",
    "one_hots = lb_title.fit_transform(range(len(vocab_chars.keys())))\n",
    "\n",
    "Y_train_title = []\n",
    "maxLenTitle = 0\n",
    "for title in clean_title:\n",
    "    Y_train_title.append(np.asarray([one_hots[vocab_chars[ch]] for ch in title]))\n",
    "    if len(title) > maxLenTitle:\n",
    "        maxLenTitle = len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_train_description = pad_sequences(X_train_description, maxlen=maxLenDesc, padding='post')\n",
    "X_train_tags = pad_sequences(X_train_tags, maxlen=maxLenTags, padding='post')\n",
    "Y_train_title = pad_sequences(Y_train_title, maxlen=maxLenTitle, padding='post', value=vocab_char_size+1)\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train_description = np.stack(X_train_description, axis=0)\n",
    "X_train_category = np.stack(X_train_category, axis=0)\n",
    "X_train_tags = np.stack(X_train_tags, axis=0)\n",
    "Y_train_title = np.stack(Y_train_title, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gen RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_genrnn(char_vocab_size,maxLenDesc,maxLenTitle,input_features,batch_size=101):\n",
    "    embedding_desc = Input(batch_input_shape=(None,None,input_features), name='main_input')\n",
    "    \n",
    "    rnn_1, state_11, state_12= LSTM(128, return_sequences=True, return_state=True, name='lstm1')(embedding_desc)\n",
    "    rnn_2, state_21, state_22 = LSTM(128, return_sequences=True, return_state=True, name='lstm2')(rnn_1)\n",
    "    \n",
    "    rnn_concat = concatenate([embedding_desc, rnn_1, rnn_2], axis=-1)\n",
    "    state_concat = concatenate([state_11, state_21], axis=-1)\n",
    "    \n",
    "    attention = Attention(name='attention')([rnn_1, rnn_2])\n",
    "    \n",
    "    output = Dense(char_vocab_size, name='output', activation='softmax')(attention)\n",
    "    \n",
    "    model = Model(inputs=[embedding_desc], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=4e-3))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         [(None, None, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    [(None, None, 128),  117248      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    [(None, None, 128),  131584      lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, None, 128)    0           lstm1[0][0]                      \n",
      "                                                                 lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, None, 456)    58824       attention[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 307,656\n",
      "Trainable params: 307,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=32\n",
    "model = text_genrnn(vocab_char_size,maxLenDesc,maxLenTitle,100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_description,Y_train_title, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
