{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Reshape, Attention, TimeDistributed, Embedding, LSTM, Bidirectional, Dropout, Dense, GRU, concatenate, Conv1D, Conv2D, Flatten,MaxPooling1D, MaxPooling2D, LocallyConnected1D, Activation, GaussianNoise, BatchNormalization, RepeatVector\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from AttentionWeightedAverage import AttentionWeightedAverage\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP on Title + Description to Tags, in order to build a tag recommendation algorithm\n",
    "- NLP on Tags + Description + Category to Title, in order to provide title recommendation algorithm. These videos are popular videos, meaning they have appealing titles. Let's build a system, that judging by content, can provide the most appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of youtube videos in dataframe:  40949\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/USvideos.csv\")\n",
    "print('Number of youtube videos in dataframe: ', len(df.index))\n",
    "df.head()\n",
    "\n",
    "# count category occurences\n",
    "occurences = df['category_id'].value_counts()\n",
    "# category names\n",
    "map_categories = {}\n",
    "with open('data/US_category_id.json') as json_file:\n",
    "    data_category = json.load(json_file)\n",
    "    for category in data_category['items']:\n",
    "        int_id = int(category['id'])\n",
    "        occ = 0\n",
    "        if int_id in occurences:\n",
    "            occ = occurences[int_id]\n",
    "        map_categories[int_id] = [category['snippet']['title'],occ]\n",
    "# create new dataframe with categories and appearances\n",
    "df_categories = pd.DataFrame.from_dict(map_categories, orient='index', columns=['category_name', 'count'])\n",
    "# add category name on the main dataframe\n",
    "for i in map_categories.keys():\n",
    "    df.loc[df['category_id'] == i,'category_name'] = map_categories[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags, Description and Category -> Auto generate Title\n",
    "We will see a word level seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(tags):\n",
    "    tags = tags.replace('\"','')\n",
    "    discrete_tags = tags.split('|')\n",
    "    return discrete_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['title', 'description', 'tags', 'category_name']]\n",
    "df_text = df_text.dropna(axis=0, how='any')\n",
    "\n",
    "titles = df_text['title'].tolist()\n",
    "descriptions = df_text['description'].tolist()\n",
    "tags = [discretize(row['tags']) for index, row in df_text.iterrows()]\n",
    "categories = df_text['category_name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Titles need to be tokenized, lowercased, categorical.\n",
    "- Descriptions need to be tokenized, lowercased, categorical.\n",
    "- Tags need to be lowercased, categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_desc = []\n",
    "clean_title = []\n",
    "for desc in descriptions:\n",
    "    desc = desc.replace('\\\\n', ' ')\n",
    "    desc = desc.lower()\n",
    "    desc = desc.replace('www.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('http.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('www.twitter.com', 'TwitterLink')\n",
    "    desc = desc.replace('www.amazon.com', 'AmazonLink')\n",
    "    desc = desc.replace('http.amzn.com', 'AmazonLink')\n",
    "    desc = desc.replace('www.instagram.com', 'InstagramLink')\n",
    "    desc = desc.replace('www.snapchat.com', 'SnapchatLink')\n",
    "    desc = desc.replace('https://bit.ly', 'PortalLink')\n",
    "    temp = text_to_word_sequence(desc, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' ')\n",
    "    for w in temp:\n",
    "        if w in stops:\n",
    "            temp.remove(w)\n",
    "    clean_desc.append(temp)\n",
    "    \n",
    "for title in titles:\n",
    "    title = title.replace('\\\\n', ' ')\n",
    "    title = title.replace('\\xa0', ' ')\n",
    "    seq = text_to_word_sequence(title, filters='\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' ')\n",
    "    clean_title.append(['START_ '] + seq + [' _END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an unsupervised method for word2vec with gensim, in order to provide me with very good embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57580053, 64013890)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "input_features = 100\n",
    "\n",
    "gensim_model = Word2Vec(size=input_features,\n",
    "                       window=5,\n",
    "                       min_count=1,\n",
    "                       sg=1,\n",
    "                       hs=0,\n",
    "                       negative=10,\n",
    "                       workers=4)\n",
    "gensim_model.build_vocab(clean_desc+clean_title+tags)\n",
    "gensim_model.train(clean_desc+clean_title+tags, total_examples=len(clean_desc+clean_title+tags), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title_words=set()\n",
    "for title in clean_title:\n",
    "    for word in title:\n",
    "        if word not in all_title_words:\n",
    "            all_title_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(gensim_model.wv.vocab)\n",
    "\n",
    "target_words = sorted(list(all_title_words))\n",
    "num_decoder_tokens = len(all_title_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating my new sequences with vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_description = []\n",
    "X_train_category = []\n",
    "X_train_tags = []\n",
    "\n",
    "lb_cat = LabelBinarizer()\n",
    "lb_cat.fit(list(set(categories)))\n",
    "\n",
    "maxLenDesc = 0\n",
    "for desc in clean_desc:\n",
    "    X_train_description.append(np.asarray([gensim_model.wv[word] for word in desc]))\n",
    "    if len(desc) > maxLenDesc:\n",
    "        maxLenDesc = len(desc)\n",
    "        \n",
    "maxLenTags = 0\n",
    "for taglist in tags:\n",
    "    X_train_tags.append(np.asarray([gensim_model.wv[tag] for tag in taglist]))\n",
    "    if len(taglist) > maxLenTags:\n",
    "        maxLenTags = len(taglist)\n",
    "\n",
    "for cat in categories:\n",
    "    X_train_category.append(lb_cat.transform([cat]))\n",
    "\n",
    "maxLenTitle = 0\n",
    "for title in clean_title:\n",
    "    if len(title) > maxLenTitle:\n",
    "        maxLenTitle = len(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the decoder inputs and outputs. Decoder target data will be ahead by one timestep, and will not include the start character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 37.9 GiB for an array with shape (40379, 22, 11444) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-03a59b7dd027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m Y_train_title = np.zeros(\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLenTitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     dtype='float32')\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# generate data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 37.9 GiB for an array with shape (40379, 22, 11444) and data type float32"
     ]
    }
   ],
   "source": [
    "X_train_title = np.zeros(\n",
    "    (len(clean_title), maxLenTitle, input_features),\n",
    "    dtype='float32')\n",
    "Y_train_title = np.zeros(\n",
    "    (len(clean_title), maxLenTitle, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# generate data\n",
    "for i, target_text in enumerate(clean_title):\n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder input data, will have the embeddings of the words\n",
    "        X_train_title[i, t] = gensim_model.wv[word]\n",
    "        if t > 0:\n",
    "            # decoder target data, will be ahead by one timestep, and will contain\n",
    "            # one hot representations of what word is selected\n",
    "            Y_train_title[i, t - 1, target_words.index(word)] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_train_description = pad_sequences(X_train_description, maxlen=maxLenDesc, padding='post')\n",
    "X_train_tags = pad_sequences(X_train_tags, maxlen=maxLenTags, padding='post')\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train_description = np.stack(X_train_description, axis=0)\n",
    "X_train_category = np.stack(X_train_category, axis=0)\n",
    "X_train_tags = np.stack(X_train_tags, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2SeqModel(vocab_size,input_max_seq,output_target_seq,input_features):\n",
    "    #encoder\n",
    "    encoder_input = Input(shape=(input_max_seq,input_features), name='main_input')\n",
    "    encoder_lstm = LSTM(100, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_input)\n",
    "    encoder_states = [encoder_state_h, encoder_state_c]\n",
    "    \n",
    "    #decoder\n",
    "    decoder_input = Input(shape=(None,input_features))\n",
    "    decoder_lstm = LSTM(100, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(vocab_size, name='decoder_dense', activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=4e-3), metrics=['accuracy'])\n",
    "    # encoder model\n",
    "    encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "    decoder_state_inputs = [Input(shape=(100,)), Input(shape=(100,))]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model,encoder_model,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "batch_size=64\n",
    "model,encoder_model,decoder_model = Seq2SeqModel(num_decoder_tokens,maxLenDesc,maxLenTitle,100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_train_description, X_train_title], Y_train_title, epochs=epochs, batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or len(decoded_sentence) > maxLenTitle):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (decode_sequence(X_train_desc[22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
