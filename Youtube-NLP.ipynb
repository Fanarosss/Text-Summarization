{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Embedding, LSTM, Bidirectional, Dropout, Dense, GRU, concatenate, Conv1D, Conv2D, Flatten,MaxPooling1D, MaxPooling2D, LocallyConnected1D, Activation, GaussianNoise, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import nltk\n",
    "from AttentionWeightedAverage import AttentionWeightedAverage\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP on Title + Description to Tags, in order to build a tag recommendation algorithm\n",
    "- NLP on Tags + Description + Category to Title, in order to provide title recommendation algorithm. These videos are popular videos, meaning they have appealing titles. Let's build a system, that judging by content, can provide the most appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of youtube videos in dataframe:  40949\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/USvideos.csv\")\n",
    "print('Number of youtube videos in dataframe: ', len(df.index))\n",
    "df.head()\n",
    "\n",
    "# count category occurences\n",
    "occurences = df['category_id'].value_counts()\n",
    "# category names\n",
    "map_categories = {}\n",
    "with open('data/US_category_id.json') as json_file:\n",
    "    data_category = json.load(json_file)\n",
    "    for category in data_category['items']:\n",
    "        int_id = int(category['id'])\n",
    "        occ = 0\n",
    "        if int_id in occurences:\n",
    "            occ = occurences[int_id]\n",
    "        map_categories[int_id] = [category['snippet']['title'],occ]\n",
    "# create new dataframe with categories and appearances\n",
    "df_categories = pd.DataFrame.from_dict(map_categories, orient='index', columns=['category_name', 'count'])\n",
    "# add category name on the main dataframe\n",
    "for i in map_categories.keys():\n",
    "    df.loc[df['category_id'] == i,'category_name'] = map_categories[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags, Description and Category -> Auto generate Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(tags):\n",
    "    tags = tags.replace('\"','')\n",
    "    discrete_tags = tags.split('|')\n",
    "    return discrete_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df[['title', 'description', 'tags', 'category_name']]\n",
    "df_text = df_text.dropna(axis=0, how='any')\n",
    "\n",
    "titles = df_text['title'].tolist()\n",
    "descriptions = df_text['description'].tolist()\n",
    "tags = [discretize(row['tags']) for index, row in df_text.iterrows()]\n",
    "categories = df_text['category_name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Titles need to be tokenized, lowercased, categorical.\n",
    "- Descriptions need to be tokenized, lowercased, categorical.\n",
    "- Tags need to be lowercased, categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_desc = []\n",
    "clean_title = []\n",
    "for desc in descriptions:\n",
    "    desc = desc.replace('\\\\n', ' ')\n",
    "    desc = desc.lower()\n",
    "    desc = desc.replace('www.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('http.facebook.com', 'FacebookLink')\n",
    "    desc = desc.replace('www.twitter.com', 'TwitterLink')\n",
    "    desc = desc.replace('www.amazon.com', 'AmazonLink')\n",
    "    desc = desc.replace('http.amzn.com', 'AmazonLink')\n",
    "    desc = desc.replace('www.instagram.com', 'InstagramLink')\n",
    "    desc = desc.replace('www.snapchat.com', 'SnapchatLink')\n",
    "    desc = desc.replace('https://bit.ly', 'PortalLink')\n",
    "    clean_desc.append(text_to_word_sequence(desc, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' '))\n",
    "    \n",
    "for title in titles:\n",
    "    title = title.replace('\\\\n', ' ')\n",
    "    clean_title.append(text_to_word_sequence(title, filters='!\"#%&()*-+,.\\\\/:;<=>?@[\\\\]^_`{|}~\\t\\n♡▶➜', lower=True, split=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an unsupervised method for word2vec with gensim, in order to provide me with very good embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68364433, 79944630)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "gensim_model = Word2Vec(size=100,\n",
    "                       window=5,\n",
    "                       min_count=1,\n",
    "                       sg=1,\n",
    "                       hs=0,\n",
    "                       negative=10,\n",
    "                       workers=4)\n",
    "gensim_model.build_vocab(clean_desc+clean_title+tags)\n",
    "gensim_model.train(clean_desc+clean_title+tags, total_examples=len(clean_desc+clean_title+tags), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating my new sequences with vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.9 GiB for an array with shape (40379, 855, 100) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-4329a6e10d0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mX_train_description\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxLenDesc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mX_train_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxLenTags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mY_train_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxLenTitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    156\u001b[0m   return sequence.pad_sequences(\n\u001b[0;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m keras_export(\n",
      "\u001b[1;32mD:\\Programming\\Anaconda\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     81\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programming\\Anaconda\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.9 GiB for an array with shape (40379, 855, 100) and data type int32"
     ]
    }
   ],
   "source": [
    "X_train_description = []\n",
    "X_train_category = []\n",
    "X_train_tags = []\n",
    "Y_train_title = []\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(set(categories)))\n",
    "\n",
    "maxLenDesc = 0\n",
    "for desc in clean_desc:\n",
    "    X_train_description.append(np.asarray([gensim_model.wv[word] for word in desc]))\n",
    "    if len(desc) > maxLenDesc:\n",
    "        maxLenDesc = len(desc)\n",
    "        \n",
    "maxLenTags = 0\n",
    "for taglist in tags:\n",
    "    X_train_tags.append(np.asarray([gensim_model.wv[tag] for tag in taglist]))\n",
    "    if len(taglist) > maxLenTags:\n",
    "        maxLenTags = len(taglist)\n",
    "\n",
    "for cat in categories:\n",
    "    X_train_tags.append(lb.transform([cat]))\n",
    "\n",
    "maxLenTitle = 0\n",
    "for title in clean_title:\n",
    "    Y_train_title.append(np.asarray([gensim_model.wv[word] for word in title]))\n",
    "    if len(title) > maxLenTitle:\n",
    "        maxLenTitle = len(title)\n",
    "        \n",
    "# padding\n",
    "X_train_description = pad_sequences(X_train_description, maxlen=maxLenDesc)\n",
    "X_train_tags = pad_sequences(X_train_tags, maxlen=maxLenTags)\n",
    "Y_train_title = pad_sequences(Y_train_title, maxlen=maxLenTitle)\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train_description = np.stack(X_train_description, axis=0)\n",
    "X_train_category = np.stack(X_train_category, axis=0)\n",
    "X_train_tags = np.stack(X_train_tags, axis=0)\n",
    "Y_train_title = np.stack(Y_train_title, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text GenRNN model, proposed by Andrej Karpathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_genrnn(vocab_size,maxLen,input_features):\n",
    "    embedding = Input(shape=(maxLen,input_features), name='main_input')\n",
    "    rnn_1 = LSTM(128)(embedding)\n",
    "    rnn_2 = LSTM(128)(rnn_1)\n",
    "    rnn_concat = concatenate(embedding, rnn_1, rnn_2)\n",
    "    attention = AttentionWeightedAverage(name='attention')(rnn_concat)\n",
    "    output = Dense(vocab_size, name='output', activation='softmax')(attention)\n",
    "    \n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=4e-3))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=32\n",
    "model.fit(X_train_description,Y_train_title,epochs=epochs,batch_size=batch_sizeverbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
